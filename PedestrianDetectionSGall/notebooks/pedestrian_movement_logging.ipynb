{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "print(\"Root Directory: \", ROOT_DIR)\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from new_mrcnn import utils\n",
    "import new_mrcnn.model as modellib\n",
    "from new_mrcnn import visualize\n",
    "from new_mrcnn.model import log\n",
    "\n",
    "from scripts import coco\n",
    "from scripts.coco import Config\n",
    "from scripts import list_file_info as lfi\n",
    "\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/1/video/\"\n",
    "frames_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/1/frames/\"\n",
    "output_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/1/output_vid/\"\n",
    "final_imgs_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/1/output_img/\"\n",
    "json_output_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/1/output_json/\"\n",
    "COCO_DIR = \"/home/sam/Desktop/cocoData/\"\n",
    "COCO_MODEL_PATH = \"/home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/weights/mask_rcnn_coco.h5\"\n",
    "MODEL_DIR = \"/home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/weights/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    #GPU_COUNT = 1\n",
    "    #IMAGES_PER_GPU = 1\n",
    "\n",
    "# Alt Config to train own model on reduced classes \n",
    "class PersonConfig(Config):\n",
    "    NAME = \"person\"\n",
    "    NUM_CLASSES = 1 + 1 # background + sheep\n",
    "\n",
    "config = PersonConfig()  # Don't forget to use this config while creating your model\n",
    "config.display()\n",
    "\n",
    "\n",
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = COCO(\"/home/sam/Desktop/cocoData/annotations/instances_train2014.json\")\n",
    "print(ct.getCatIds(['BG']))\n",
    "print(ct.getCatIds(['person']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = coco.CocoDataset()\n",
    "dataset_train.load_coco(COCO_DIR, \"train\", class_ids=[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = coco.CocoDataset()\n",
    "dataset_val.load_coco(COCO_DIR, \"val\", class_ids=[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(dataset_train, dataset_val,\n",
    "        learning_rate=config.LEARNING_RATE, \n",
    "        epochs=100, \n",
    "        layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"weights/mask_rcnn_coco.h5\")\n",
    "\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "class_names = ['BG','person']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "print(\"Loading weights \", COCO_MODEL_PATH)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection for frame names in frames_dir\n",
    "\n",
    "frames_list = lfi.get_file_names(frames_dir)\n",
    "\n",
    "# Sort the frames into accending order\n",
    "\n",
    "frames_list.sort(key=lambda f: int(re.sub('\\D', '', f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_annos = dict()\n",
    "\n",
    "\n",
    "for i in range(9):\n",
    "    \n",
    "    image_path = os.path.join(frames_dir,frames_list[i]) \n",
    "    print(image_path)\n",
    "    image = cv2.imread(image_path)\n",
    "    _, ax = plt.subplots(1, figsize=(26, 26))\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    results = model.detect([image], verbose=1)\n",
    "    \n",
    "    r = results[0]\n",
    "    \n",
    "    image_ir = visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                class_names, r['scores'], ax=ax,\n",
    "                                title=\"Predictions\")\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir,frames_list[i]), bbox_inches='tight')\n",
    "    \n",
    "    test_annos[i] = {frames_list[i] : {'objects': ([class_names[predicted_class_name] for predicted_class_name in r['class_ids']], \n",
    "                                                r['scores'].tolist(),  \n",
    "                                                r['rois'].tolist(),\n",
    "                                                class_names,\n",
    "                                                width, height)}}\n",
    "    if i==9:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfdfb253d56b3adda966a24c23cbff432cd99a2a96e5b72ef76cc71805036f62"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
