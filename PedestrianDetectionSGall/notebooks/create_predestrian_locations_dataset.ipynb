{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection person locations within frame and building a csv dataset\n",
    "\n",
    "### Notebook 3 \n",
    "\n",
    "In this notebook I will take the progress in using the model to identify pedestrians within frame and build a datat set of pedestrian location within a single frame.\n",
    "\n",
    "By logging the coordinates within the following frame I'll attempt to link that coordinates that have moved very slightly will likely be from the pedestrian in the previous frame.\n",
    "\n",
    "From this I will build a csv data set to use for future location predicition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Directory:  /home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "import re\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "print(\"Root Directory: \", ROOT_DIR)\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "from scripts import coco\n",
    "from scripts import list_file_info as lfi\n",
    "from scripts import video_to_frames as vtf\n",
    "from scripts.centroid_tracker import CentroidTracker\n",
    "from scripts.trackable_object import TrackableObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please select video from 1 to 4\n"
     ]
    }
   ],
   "source": [
    "data_root = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/\"\n",
    "\n",
    "file_list = os.listdir(path = data_root) \n",
    "max = len(file_list) \n",
    "\n",
    "if max == 0:\n",
    "    print(\"No video folders in directory\")\n",
    "else:\n",
    "    print(\"please select video from 1 to \" + str(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a video \n",
    "\n",
    "selected_video = 4\n",
    "\n",
    "vid_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/\" + str(selected_video) + \"/video/\"\n",
    "frames_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/\" + str(selected_video) + \"/frames/\"\n",
    "final_vid_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/\" + str(selected_video) + \"/output_vid/\"\n",
    "output_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/\" + str(selected_video) + \"/output_img/\"\n",
    "json_output_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/\" + str(selected_video) + \"/output_json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                93\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           coco\n",
      "NUM_CLASSES                    81\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"weights/mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "                'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "                'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "                'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "                'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "                'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "                'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "                'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "                'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "                'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "                'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "                'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "                'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "                'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "                'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  /home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/weights/mask_rcnn_coco.h5\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "print(\"Loading weights \", COCO_MODEL_PATH)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sam/Desktop/sam_bdd_videos/videos_for_frames/4/video/73a4bd1e-bfe891e6.mov\n"
     ]
    }
   ],
   "source": [
    "# Get video file name \n",
    "\n",
    "vid_name = lfi.get_file_name(vid_dir)\n",
    "\n",
    "video_path = vid_dir + vid_name\n",
    "print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames already exist\n",
      "You can select from frame 0 to 1201\n"
     ]
    }
   ],
   "source": [
    "# Check if frames exist and if not break video into frames\n",
    "\n",
    "frame_list = os.listdir(path = frames_dir) \n",
    "max = len(frame_list) \n",
    "\n",
    "if max == 0:\n",
    "    vtf.getFrames(video_path, frames_dir)\n",
    "    print(\"Video broken down to frames\")\n",
    "    frame_list = os.listdir(path = frames_dir)\n",
    "    max = len(frame_list) \n",
    "else:\n",
    "    print(\"frames already exist\")\n",
    "\n",
    "print(\"You can select from frame 0 to \" + str(max - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection for frame names in frames_dir\n",
    "\n",
    "frames_list = lfi.get_file_names(frames_dir)\n",
    "\n",
    "# Sort the frames into accending order\n",
    "\n",
    "frames_list.sort(key=lambda f: int(re.sub('\\D', '', f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'frame 540': {0: {'pedestrian 0': ('Y1 = ', 200, 'X1 = ', 1014, 'Y2 = ', 458, 'X2 = ', 1134, 'Accuracy = ', 0.9991887)}, 1: {'pedestrian 1': ('Y1 = ', 174, 'X1 = ', 1141, 'Y2 = ', 455, 'X2 = ', 1259, 'Accuracy = ', 0.9989993)}, 3: {'pedestrian 2': ('Y1 = ', 142, 'X1 = ', 1241, 'Y2 = ', 423, 'X2 = ', 1280, 'Accuracy = ', 0.9822912)}}}\n",
      "{'frame 541': {0: {'pedestrian 0': ('Y1 = ', 174, 'X1 = ', 1139, 'Y2 = ', 450, 'X2 = ', 1250, 'Accuracy = ', 0.9989153)}, 1: {'pedestrian 1': ('Y1 = ', 201, 'X1 = ', 1010, 'Y2 = ', 454, 'X2 = ', 1129, 'Accuracy = ', 0.9987388)}, 3: {'pedestrian 2': ('Y1 = ', 150, 'X1 = ', 1241, 'Y2 = ', 425, 'X2 = ', 1279, 'Accuracy = ', 0.9820364)}}}\n",
      "{'frame 542': {0: {'pedestrian 0': ('Y1 = ', 175, 'X1 = ', 1135, 'Y2 = ', 450, 'X2 = ', 1246, 'Accuracy = ', 0.99939036)}, 1: {'pedestrian 1': ('Y1 = ', 202, 'X1 = ', 1012, 'Y2 = ', 455, 'X2 = ', 1127, 'Accuracy = ', 0.99861896)}, 2: {'pedestrian 2': ('Y1 = ', 147, 'X1 = ', 1237, 'Y2 = ', 421, 'X2 = ', 1278, 'Accuracy = ', 0.99242544)}}}\n",
      "{'frame 543': {0: {'pedestrian 0': ('Y1 = ', 177, 'X1 = ', 1132, 'Y2 = ', 449, 'X2 = ', 1241, 'Accuracy = ', 0.9994733)}, 1: {'pedestrian 1': ('Y1 = ', 208, 'X1 = ', 1010, 'Y2 = ', 454, 'X2 = ', 1121, 'Accuracy = ', 0.998104)}, 2: {'pedestrian 2': ('Y1 = ', 148, 'X1 = ', 1234, 'Y2 = ', 427, 'X2 = ', 1279, 'Accuracy = ', 0.9938121)}}}\n"
     ]
    }
   ],
   "source": [
    "frame_path_1 = os.path.join(frames_dir,frames_list[540])\n",
    "frame_path_2 = os.path.join(frames_dir,frames_list[541])\n",
    "frame_path_3 = os.path.join(frames_dir,frames_list[542])\n",
    "frame_path_4 = os.path.join(frames_dir,frames_list[543])\n",
    "\n",
    "frame_paths = [frame_path_1, frame_path_2, frame_path_3, frame_path_4]\n",
    "\n",
    "frames_dict = dict()\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "for frame_path in frame_paths:\n",
    "    \n",
    "    image = cv2.imread(frame_path)\n",
    "    \n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    \n",
    "    # Display results\n",
    "    r = results[0]\n",
    "    \n",
    "    objects_dict = dict()\n",
    "    \n",
    "    objects_count = 0\n",
    "    \n",
    "    pedestrian_count = 0\n",
    "    \n",
    "    for id in r['class_ids']:\n",
    "        \n",
    "        if id == 1:\n",
    "            \n",
    "            objects_dict[objects_count] = {'pedestrian ' + str(pedestrian_count): ('Y1 = ', r['rois'][objects_count][0],\n",
    "                                                                            'X1 = ', r['rois'][objects_count][1],\n",
    "                                                                            'Y2 = ', r['rois'][objects_count][2],\n",
    "                                                                            'X2 = ', r['rois'][objects_count][3], \n",
    "                                                                            'Accuracy = ', r['scores'][objects_count])}\n",
    "            \n",
    "            pedestrian_count += 1\n",
    "        \n",
    "        objects_count += 1\n",
    "    \n",
    "    frame_number = frame_path.replace(\"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/4/frames/\", \"\").replace(\".jpg\", \"\")\n",
    "    \n",
    "    frames_dict[frame_count] = {'frame ' + frame_number : objects_dict}\n",
    "    \n",
    "    frame_count += 1\n",
    "\n",
    "\n",
    "for fd in frames_dict:\n",
    "    print(frames_dict[fd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will try and get cell above working with centroid driver and centroid tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters\n",
    "confThreshold = 0.8  #Confidence threshold\n",
    "nmsThreshold = 0.4   #Non-maximum suppression threshold\n",
    "inpWidth = 416       #Width of network's input image\n",
    "inpHeight = 416      #Height of network's input image\n",
    "\n",
    "# initialize the video writer\n",
    "writer = None\n",
    "\n",
    "# initialize the frame dimensions (we'll set them as soon as we read\n",
    "# the first frame from the video)\n",
    "W = None\n",
    "H = None\n",
    "\n",
    "ct = CentroidTracker(maxDisappeared=40, maxDistance=50)\n",
    "trackers = []\n",
    "trackableObjects = {}\n",
    "\n",
    "# initialize the total number of frames processed thus far, along\n",
    "# with the total number of objects that have moved either up or down\n",
    "totalDown = 0\n",
    "totalUp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(0, array([ 767, 1243])), (1, array([ 803, 1368])), (2, array([ 803, 1368])), (3, array([ 799, 1364])), (4, array([ 765, 1237])), (5, array([ 765, 1237])), (6, array([ 798, 1360])), (7, array([ 765, 1239])), (8, array([ 765, 1239])), (9, array([ 797, 1356])), (10, array([ 768, 1237])), (11, array([ 768, 1237]))])\n"
     ]
    }
   ],
   "source": [
    "frame_path_1 = os.path.join(frames_dir,frames_list[540])\n",
    "frame_path_2 = os.path.join(frames_dir,frames_list[541])\n",
    "frame_path_3 = os.path.join(frames_dir,frames_list[542])\n",
    "frame_path_4 = os.path.join(frames_dir,frames_list[543])\n",
    "\n",
    "frame_paths = [frame_path_1, frame_path_2, frame_path_3, frame_path_4]\n",
    "\n",
    "rects = []\n",
    "\n",
    "for frame_path in frame_paths:\n",
    "    \n",
    "    objects_count = 0\n",
    "    \n",
    "    image = cv2.imread(frame_path)\n",
    "    \n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    \n",
    "    # Display results\n",
    "    r = results[0]\n",
    "    \n",
    "    for id in r['class_ids']:\n",
    "        \n",
    "        if id == 1:\n",
    "            \n",
    "            if r['scores'][objects_count] > confThreshold:\n",
    "                \n",
    "                # x1 = Top\n",
    "                top = r['rois'][objects_count][1]\n",
    "                # y1 = left\n",
    "                left = r['rois'][objects_count][0]\n",
    "                # x2 = width\n",
    "                width = r['rois'][objects_count][3]\n",
    "                # y2 = height\n",
    "                height = r['rois'][objects_count][2]\n",
    "                \n",
    "                rects.append([left, top, left + width, top + height])\n",
    "                \n",
    "        objects_count =+ 1\n",
    "    \n",
    "    objects = ct.update(rects)\n",
    "    \n",
    "\n",
    "print(objects)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement this code\n",
    "\n",
    "def counting(objects):\n",
    "    frameHeight = frame.shape[0]\n",
    "    frameWidth = frame.shape[1]\n",
    "\n",
    "    global totalDown\n",
    "    global totalUp\n",
    "\n",
    "    # loop over the tracked objects\n",
    "    for (objectID, centroid) in objects.items():\n",
    "        # check to see if a trackable object exists for the current\n",
    "        # object ID\n",
    "        to = trackableObjects.get(objectID, None)\n",
    "\n",
    "        # if there is no existing trackable object, create one\n",
    "        if to is None:\n",
    "            to = TrackableObject(objectID, centroid)\n",
    "\n",
    "        # otherwise, there is a trackable object so we can utilize it\n",
    "        # to determine direction\n",
    "        else:\n",
    "            # the difference between the y-coordinate of the *current*\n",
    "            # centroid and the mean of *previous* centroids will tell\n",
    "            # us in which direction the object is moving (negative for\n",
    "            # 'up' and positive for 'down')\n",
    "            y = [c[1] for c in to.centroids]\n",
    "            direction = centroid[1] - np.mean(y)\n",
    "            print(direction)\n",
    "            to.centroids.append(centroid)\n",
    "\n",
    "            # check to see if the object has been counted or not\n",
    "            if not to.counted:\n",
    "                # if the direction is negative (indicating the object\n",
    "                # is moving up) AND the centroid is above the center\n",
    "                # line, count the object\n",
    "\n",
    "                if direction < 0 and centroid[1] in range(frameHeight//2 - 30, frameHeight//2 + 30):\n",
    "                    totalUp += 1\n",
    "                    to.counted = True\n",
    "\n",
    "                # if the direction is positive (indicating the object\n",
    "                # is moving down) AND the centroid is below the\n",
    "                # center line, count the object\n",
    "                elif direction > 0 and centroid[1] in range(frameHeight//2 - 30, frameHeight//2 + 30):\n",
    "                    totalDown += 1\n",
    "                    to.counted = True\n",
    "\n",
    "        # store the trackable object in our dictionary\n",
    "        trackableObjects[objectID] = to\n",
    "        # draw both the ID of the object and the centroid of the\n",
    "        # object on the output frame\n",
    "        #text = \"ID {}\".format(objectID)\n",
    "        #cv.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n",
    "            #cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
    "    # construct a tuple of information we will be displaying on the\n",
    "    # frame\n",
    "    info = [\n",
    "        (\"Up\", totalUp),\n",
    "        (\"Down\", totalDown),\n",
    "    ]\n",
    "\n",
    "    # loop over the info tuples and draw them on our frame\n",
    "    for (i, (k, v)) in enumerate(info):\n",
    "        text = \"{}: {}\".format(k, v)\n",
    "        cv.putText(frame, text, (10, frameHeight - ((i * 20) + 20)),\n",
    "            cv.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfdfb253d56b3adda966a24c23cbff432cd99a2a96e5b72ef76cc71805036f62"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
