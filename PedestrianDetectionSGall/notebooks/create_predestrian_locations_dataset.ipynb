{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection person locations within frame and building a csv dataset\n",
    "\n",
    "### Notebook 3 \n",
    "\n",
    "In this notebook I will take the progress in using the model to identify pedestrians within frame and build a datat set of pedestrian location within a single frame.\n",
    "\n",
    "By logging the coordinates within the following frame I'll attempt to link that coordinates that have moved very slightly will likely be from the pedestrian in the previous frame.\n",
    "\n",
    "From this I will build a csv data set to use for future location predicition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Directory:  /home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "import re\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "print(\"Root Directory: \", ROOT_DIR)\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "from scripts import coco\n",
    "from scripts import list_file_info as lfi\n",
    "from scripts import video_to_frames as vtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please select video from 1 to 4\n"
     ]
    }
   ],
   "source": [
    "data_root = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/\"\n",
    "\n",
    "file_list = os.listdir(path = data_root) \n",
    "max = len(file_list) \n",
    "\n",
    "if max == 0:\n",
    "    print(\"No video folders in directory\")\n",
    "else:\n",
    "    print(\"please select video from 1 to \" + str(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a video \n",
    "\n",
    "selected_video = 4\n",
    "\n",
    "vid_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/\" + str(selected_video) + \"/video/\"\n",
    "frames_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/\" + str(selected_video) + \"/frames/\"\n",
    "final_vid_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/\" + str(selected_video) + \"/output_vid/\"\n",
    "output_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/\" + str(selected_video) + \"/output_img/\"\n",
    "json_output_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/\" + str(selected_video) + \"/output_json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                93\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           coco\n",
      "NUM_CLASSES                    81\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"weights/mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "                'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "                'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "                'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "                'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "                'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "                'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "                'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "                'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "                'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "                'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "                'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "                'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "                'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "                'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 20:04:31.395461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-28 20:04:31.490875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-28 20:04:31.491044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-28 20:04:31.492704: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-28 20:04:31.493375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-28 20:04:31.493510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-28 20:04:31.493627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-28 20:04:32.408949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-28 20:04:32.409248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-28 20:04:32.409507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-28 20:04:32.410534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21541 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:0a:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sam/anaconda3/envs/mrcnn/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "Loading weights  /home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/weights/mask_rcnn_coco.h5\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "print(\"Loading weights \", COCO_MODEL_PATH)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sam/Desktop/sam_bdd_videos/videos_for_frames/4/video/73a4bd1e-bfe891e6.mov\n"
     ]
    }
   ],
   "source": [
    "# Get video file name \n",
    "\n",
    "vid_name = lfi.get_file_name(vid_dir)\n",
    "\n",
    "video_path = vid_dir + vid_name\n",
    "print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames already exist\n",
      "You can select from frame 0 to 1201\n"
     ]
    }
   ],
   "source": [
    "# Check if frames exist and if not break video into frames\n",
    "\n",
    "frame_list = os.listdir(path = frames_dir) \n",
    "max = len(frame_list) \n",
    "\n",
    "if max == 0:\n",
    "    vtf.getFrames(video_path, frames_dir)\n",
    "    print(\"Video broken down to frames\")\n",
    "    frame_list = os.listdir(path = frames_dir)\n",
    "    max = len(frame_list) \n",
    "else:\n",
    "    print(\"frames already exist\")\n",
    "\n",
    "print(\"You can select from frame 0 to \" + str(max - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection for frame names in frames_dir\n",
    "\n",
    "frames_list = lfi.get_file_names(frames_dir)\n",
    "\n",
    "# Sort the frames into accending order\n",
    "\n",
    "frames_list.sort(key=lambda f: int(re.sub('\\D', '', f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1014, 200, 1134, 458]\n",
      "[[1014, 200, 1134, 458]]\n",
      "[1141, 174, 1259, 455]\n",
      "[[1014, 200, 1134, 458], [1141, 174, 1259, 455]]\n",
      "[1241, 142, 1280, 423]\n",
      "[[1014, 200, 1134, 458], [1141, 174, 1259, 455], [1241, 142, 1280, 423]]\n",
      "[1139, 174, 1250, 450]\n",
      "[[1014, 200, 1134, 458], [1141, 174, 1259, 455], [1241, 142, 1280, 423], [1139, 174, 1250, 450]]\n",
      "[1010, 201, 1129, 454]\n",
      "[[1014, 200, 1134, 458], [1141, 174, 1259, 455], [1241, 142, 1280, 423], [1139, 174, 1250, 450], [1010, 201, 1129, 454]]\n",
      "[1241, 150, 1279, 425]\n",
      "[[1014, 200, 1134, 458], [1141, 174, 1259, 455], [1241, 142, 1280, 423], [1139, 174, 1250, 450], [1010, 201, 1129, 454], [1241, 150, 1279, 425]]\n",
      "[1135, 175, 1246, 450]\n",
      "[[1014, 200, 1134, 458], [1141, 174, 1259, 455], [1241, 142, 1280, 423], [1139, 174, 1250, 450], [1010, 201, 1129, 454], [1241, 150, 1279, 425], [1135, 175, 1246, 450]]\n",
      "[1012, 202, 1127, 455]\n",
      "[[1014, 200, 1134, 458], [1141, 174, 1259, 455], [1241, 142, 1280, 423], [1139, 174, 1250, 450], [1010, 201, 1129, 454], [1241, 150, 1279, 425], [1135, 175, 1246, 450], [1012, 202, 1127, 455]]\n",
      "[1237, 147, 1278, 421]\n",
      "[[1014, 200, 1134, 458], [1141, 174, 1259, 455], [1241, 142, 1280, 423], [1139, 174, 1250, 450], [1010, 201, 1129, 454], [1241, 150, 1279, 425], [1135, 175, 1246, 450], [1012, 202, 1127, 455], [1237, 147, 1278, 421]]\n",
      "[1132, 177, 1241, 449]\n",
      "[[1014, 200, 1134, 458], [1141, 174, 1259, 455], [1241, 142, 1280, 423], [1139, 174, 1250, 450], [1010, 201, 1129, 454], [1241, 150, 1279, 425], [1135, 175, 1246, 450], [1012, 202, 1127, 455], [1237, 147, 1278, 421], [1132, 177, 1241, 449]]\n",
      "[1010, 208, 1121, 454]\n",
      "[[1014, 200, 1134, 458], [1141, 174, 1259, 455], [1241, 142, 1280, 423], [1139, 174, 1250, 450], [1010, 201, 1129, 454], [1241, 150, 1279, 425], [1135, 175, 1246, 450], [1012, 202, 1127, 455], [1237, 147, 1278, 421], [1132, 177, 1241, 449], [1010, 208, 1121, 454]]\n",
      "[1234, 148, 1279, 427]\n",
      "[[1014, 200, 1134, 458], [1141, 174, 1259, 455], [1241, 142, 1280, 423], [1139, 174, 1250, 450], [1010, 201, 1129, 454], [1241, 150, 1279, 425], [1135, 175, 1246, 450], [1012, 202, 1127, 455], [1237, 147, 1278, 421], [1132, 177, 1241, 449], [1010, 208, 1121, 454], [1234, 148, 1279, 427]]\n",
      "OrderedDict([(0, array([1074,  329])), (1, array([1200,  314])), (2, array([1260,  282])), (3, array([1194,  312])), (4, array([1069,  327])), (5, array([1260,  287])), (6, array([1257,  284])), (7, array([1190,  312])), (8, array([1069,  328])), (9, array([1186,  313])), (10, array([1065,  331])), (11, array([1256,  287]))])\n",
      "{'frame 540': {0: {'pedestrian 0': ('Y1 = ', 200, 'X1 = ', 1014, 'Y2 = ', 458, 'X2 = ', 1134, 'Accuracy = ', 0.9991887)}, 1: {'pedestrian 1': ('Y1 = ', 174, 'X1 = ', 1141, 'Y2 = ', 455, 'X2 = ', 1259, 'Accuracy = ', 0.9989993)}, 3: {'pedestrian 2': ('Y1 = ', 142, 'X1 = ', 1241, 'Y2 = ', 423, 'X2 = ', 1280, 'Accuracy = ', 0.9822954)}}}\n",
      "{'frame 541': {0: {'pedestrian 0': ('Y1 = ', 174, 'X1 = ', 1139, 'Y2 = ', 450, 'X2 = ', 1250, 'Accuracy = ', 0.9989153)}, 1: {'pedestrian 1': ('Y1 = ', 201, 'X1 = ', 1010, 'Y2 = ', 454, 'X2 = ', 1129, 'Accuracy = ', 0.9987388)}, 3: {'pedestrian 2': ('Y1 = ', 150, 'X1 = ', 1241, 'Y2 = ', 425, 'X2 = ', 1279, 'Accuracy = ', 0.9820364)}}}\n",
      "{'frame 542': {0: {'pedestrian 0': ('Y1 = ', 175, 'X1 = ', 1135, 'Y2 = ', 450, 'X2 = ', 1246, 'Accuracy = ', 0.99939036)}, 1: {'pedestrian 1': ('Y1 = ', 202, 'X1 = ', 1012, 'Y2 = ', 455, 'X2 = ', 1127, 'Accuracy = ', 0.99861896)}, 2: {'pedestrian 2': ('Y1 = ', 147, 'X1 = ', 1237, 'Y2 = ', 421, 'X2 = ', 1278, 'Accuracy = ', 0.99242544)}}}\n",
      "{'frame 543': {0: {'pedestrian 0': ('Y1 = ', 177, 'X1 = ', 1132, 'Y2 = ', 449, 'X2 = ', 1241, 'Accuracy = ', 0.9994733)}, 1: {'pedestrian 1': ('Y1 = ', 208, 'X1 = ', 1010, 'Y2 = ', 454, 'X2 = ', 1121, 'Accuracy = ', 0.998104)}, 2: {'pedestrian 2': ('Y1 = ', 148, 'X1 = ', 1234, 'Y2 = ', 427, 'X2 = ', 1279, 'Accuracy = ', 0.9938121)}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from scripts.centroid_tracker import CentroidTracker\n",
    "\n",
    "ct = CentroidTracker(maxDisappeared=40, maxDistance=50)\n",
    "(H, W) = (None, None)\n",
    "\n",
    "frame_path_1 = os.path.join(frames_dir,frames_list[540])\n",
    "frame_path_2 = os.path.join(frames_dir,frames_list[541])\n",
    "frame_path_3 = os.path.join(frames_dir,frames_list[542])\n",
    "frame_path_4 = os.path.join(frames_dir,frames_list[543])\n",
    "\n",
    "frame_paths = [frame_path_1, frame_path_2, frame_path_3, frame_path_4]\n",
    "\n",
    "frames_dict = dict()\n",
    "\n",
    "tracked_objects = []\n",
    "\n",
    "bounding_boxes_array = []\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "for frame_path in frame_paths:\n",
    "    \n",
    "    image = cv2.imread(frame_path)\n",
    "    \n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    \n",
    "    # Display results\n",
    "    r = results[0]\n",
    "    \n",
    "    objects_dict = dict()\n",
    "    \n",
    "    objects_count = 0\n",
    "    \n",
    "    pedestrian_count = 0\n",
    "    \n",
    "    for id in r['class_ids']:\n",
    "        \n",
    "        if id == 1:\n",
    "            \n",
    "            objects_dict[objects_count] = {'pedestrian ' + str(pedestrian_count): ('Y1 = ', r['rois'][objects_count][0],\n",
    "                                                                            'X1 = ', r['rois'][objects_count][1],\n",
    "                                                                            'Y2 = ', r['rois'][objects_count][2],\n",
    "                                                                            'X2 = ', r['rois'][objects_count][3], \n",
    "                                                                            'Accuracy = ', r['scores'][objects_count])}\n",
    "            \n",
    "            box = [r['rois'][objects_count][1], r['rois'][objects_count][0], r['rois'][objects_count][3], r['rois'][objects_count][2]] \n",
    "            \n",
    "            print(box)\n",
    "            \n",
    "            bounding_boxes_array.append(box)\n",
    "            \n",
    "            print(bounding_boxes_array)\n",
    "            \n",
    "            pedestrian_count += 1\n",
    "        \n",
    "        objects_count += 1\n",
    "        \n",
    "    tracked_objects = ct.update(bounding_boxes_array)\n",
    "    \n",
    "    frame_number = frame_path.replace(\"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/4/frames/\", \"\").replace(\".jpg\", \"\")\n",
    "    \n",
    "    frames_dict[frame_count] = {'frame ' + frame_number : objects_dict}\n",
    "    \n",
    "    frame_count += 1\n",
    "\n",
    "print(tracked_objects)\n",
    "\n",
    "for fd in frames_dict:\n",
    "    print(frames_dict[fd])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfdfb253d56b3adda966a24c23cbff432cd99a2a96e5b72ef76cc71805036f62"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
