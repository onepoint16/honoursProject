{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection person locations within frame and building a csv dataset\n",
    "\n",
    "### Notebook 3 \n",
    "\n",
    "In this notebook I will take the progress in using the model to identify pedestrians within frame and build a datat set of pedestrian location within a single frame.\n",
    "\n",
    "By logging the coordinates within the following frame I'll attempt to link that coordinates that have moved very slightly will likely be from the pedestrian in the previous frame.\n",
    "\n",
    "From this I will build a csv data set to use for future location predicition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "import re\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "print(\"Root Directory: \", ROOT_DIR)\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "from scripts import coco\n",
    "from scripts import list_file_info as lfi\n",
    "from scripts import video_to_frames as vtf\n",
    "from scripts.centroid_tracker import CentroidTracker\n",
    "from scripts.trackable_object import TrackableObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/sam/Documents/GitHub/honoursProject/\"\n",
    "\n",
    "file_list = os.listdir(path = data_root) \n",
    "max = len(file_list) \n",
    "\n",
    "if max == 0:\n",
    "    print(\"No video folders in directory\")\n",
    "else:\n",
    "    print(\"please select video from 1 to \" + str(max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a video \n",
    "\n",
    "selected_video = 4\n",
    "\n",
    "vid_dir = \"/home/sam/Documents/GitHub/honoursProject/data/video/\"\n",
    "frames_dir = \"/home/sam/Documents/GitHub/honoursProject/data/frames/\"\n",
    "final_vid_dir = \"/home/sam/Documents/GitHub/honoursProject/data/output_vid/\"\n",
    "output_2_dir = \"/home/sam/Documents/GitHub/honoursProject/data/output_image_2_frame/\"\n",
    "output_5_dir = \"/home/sam/Documents/GitHub/honoursProject/data/output_image_5_frame/\"\n",
    "output_10_dir = \"/home/sam/Documents/GitHub/honoursProject/data/output_image_10_frame/\"\n",
    "json_output_dir = \"/home/sam/Documents/GitHub/honoursProject/data/output_json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"weights/mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "                'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "                'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "                'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "                'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "                'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "                'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "                'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "                'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "                'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "                'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "                'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "                'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "                'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "                'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "print(\"Loading weights \", COCO_MODEL_PATH)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get video file name \n",
    "\n",
    "vid_name = lfi.get_file_name(vid_dir)\n",
    "\n",
    "video_path = vid_dir + vid_name\n",
    "print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if frames exist and if not break video into frames\n",
    "\n",
    "frame_list = os.listdir(path = frames_dir) \n",
    "max = len(frame_list) \n",
    "\n",
    "if max == 0:\n",
    "    vtf.getFrames(video_path, frames_dir)\n",
    "    print(\"Video broken down to frames\")\n",
    "    frame_list = os.listdir(path = frames_dir)\n",
    "    max = len(frame_list) \n",
    "else:\n",
    "    print(\"frames already exist\")\n",
    "\n",
    "print(\"You can select from frame 0 to \" + str(max - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection for frame names in frames_dir\n",
    "\n",
    "frames_list = lfi.get_file_names(frames_dir)\n",
    "\n",
    "# Sort the frames into accending order\n",
    "\n",
    "frames_list.sort(key=lambda f: int(re.sub('\\D', '', f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create paths to frames \n",
    "frame_path_1 = os.path.join(frames_dir,frames_list[540])\n",
    "frame_path_2 = os.path.join(frames_dir,frames_list[542])\n",
    "frame_path_3 = os.path.join(frames_dir,frames_list[544])\n",
    "frame_path_4 = os.path.join(frames_dir,frames_list[546])\n",
    "frame_path_5 = os.path.join(frames_dir,frames_list[548])\n",
    "frame_path_6 = os.path.join(frames_dir,frames_list[550])\n",
    "frame_path_7 = os.path.join(frames_dir,frames_list[552])\n",
    "frame_path_8 = os.path.join(frames_dir,frames_list[554])\n",
    "frame_path_9 = os.path.join(frames_dir,frames_list[556])\n",
    "frame_path_10 = os.path.join(frames_dir,frames_list[558])\n",
    "frame_path_11 = os.path.join(frames_dir,frames_list[560])\n",
    "frame_path_12 = os.path.join(frames_dir,frames_list[562])\n",
    "frame_path_13 = os.path.join(frames_dir,frames_list[564])\n",
    "frame_path_14 = os.path.join(frames_dir,frames_list[566])\n",
    "\n",
    "# Create frames directory\n",
    "frame_paths = [frame_path_1, frame_path_2, frame_path_3, frame_path_4, frame_path_5, \n",
    "                frame_path_6, frame_path_7, frame_path_8, frame_path_9, frame_path_10, \n",
    "                frame_path_11, frame_path_12, frame_path_13, frame_path_14]\n",
    "\n",
    "# Create frames dictionary to hold each frame and the pedestrians within it\n",
    "frames_dict = dict()\n",
    "\n",
    "# Frame counter \n",
    "frame_count = 0\n",
    "\n",
    "# Begin loop through frames in frame directory\n",
    "for frame_path in frame_paths:\n",
    "    \n",
    "    # Set plot details for visulisation\n",
    "    _, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    \n",
    "    # Open cv analyses frame data\n",
    "    frame = cv2.imread(frame_path)\n",
    "    \n",
    "    # Run object detection\n",
    "    results = model.detect([frame], verbose=0)\n",
    "    \n",
    "    # collect results\n",
    "    r = results[0]\n",
    "    \n",
    "    # Collect labels for display\n",
    "    labels =[]\n",
    "    \n",
    "    # Create object dictionary to hold objects detected within results\n",
    "    objects_dict = dict()\n",
    "    \n",
    "    # Objects counter\n",
    "    objects_count = 0\n",
    "    \n",
    "    # Pedestrian counter\n",
    "    pedestrian_count = 0\n",
    "    \n",
    "    # Loop through all identified class id's \n",
    "    for id in r['class_ids']:\n",
    "        \n",
    "        # If class id is equal to 1 its a person\n",
    "        if id == 1:\n",
    "            \n",
    "            # Instantiate Centroid Tracker class\n",
    "            ct = CentroidTracker()\n",
    "            \n",
    "            # Add person plus person count to labels for visulisation\n",
    "            labels.append(str(pedestrian_count))\n",
    "            \n",
    "            # create collection for coordiunates for centroid calculation\n",
    "            rects = []\n",
    "            \n",
    "            # Set Coordinates of pedestrian for centroid calculation\n",
    "            # x1 = top\n",
    "            top = r['rois'][objects_count][1]\n",
    "            # y1 = left\n",
    "            left = r['rois'][objects_count][0]\n",
    "            # x2 = width\n",
    "            width = r['rois'][objects_count][3]\n",
    "            # y2 = height\n",
    "            height = r['rois'][objects_count][2]\n",
    "            \n",
    "            # Add coordinates to rects \n",
    "            rects.append([left, top, left + width, top + height])\n",
    "            \n",
    "            # Get centroid results for pedestrian returns: (id, array([ x, y])) id not working so ignore\n",
    "            centroid = ct.update(rects)\n",
    "            \n",
    "            # Get centroid X and Y\n",
    "            X = centroid[0][0]\n",
    "            Y = centroid[0][1]\n",
    "            \n",
    "            # Add person object to objects dictionary with centroid coordinates\n",
    "            objects_dict[objects_count] = {'pedestrian ' + str(pedestrian_count): (X, Y)}\n",
    "            \n",
    "            # Add 1 to pedestrian total count\n",
    "            pedestrian_count += 1\n",
    "        \n",
    "        # Else conditional for all other objects detected\n",
    "        else:\n",
    "            # Adds empty string to labels\n",
    "            labels.append(\"\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Add 1 to objects total count\n",
    "        objects_count += 1\n",
    "    \n",
    "    # Concatonate the frame number from frame path \n",
    "    frame_number = frame_path.replace(\"/home/sam/Documents/GitHub/honoursProject/4/frames/\", \"\").replace(\".jpg\", \"\")\n",
    "    \n",
    "    # Add frame with frame number to frames dictionary with the objects dictionary nested\n",
    "    frames_dict[frame_count] = {'frame ' + frame_number : objects_dict}\n",
    "\n",
    "    image_ir = visualize.display_instances(frame, r['rois'], r['masks'], r['class_ids'], \n",
    "                                class_names, r['scores'], ax=ax,\n",
    "                                title=\"Frame \" + str(frame_number), captions=labels)\n",
    "    \n",
    "    plt.savefig(os.path.join(output_2_dir,frame_number), bbox_inches='tight')\n",
    "    \n",
    "    # Add 1 to frames total count\n",
    "    frame_count += 1\n",
    "\n",
    "# Loop through frames dictionary\n",
    "for fd in frames_dict:\n",
    "    # Print the frame data for each dictionary\n",
    "    print(frames_dict[fd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create paths to frames \n",
    "frame_path_1 = os.path.join(frames_dir,frames_list[540])\n",
    "frame_path_2 = os.path.join(frames_dir,frames_list[545])\n",
    "frame_path_3 = os.path.join(frames_dir,frames_list[550])\n",
    "frame_path_4 = os.path.join(frames_dir,frames_list[555])\n",
    "frame_path_5 = os.path.join(frames_dir,frames_list[560])\n",
    "frame_path_6 = os.path.join(frames_dir,frames_list[565])\n",
    "frame_path_7 = os.path.join(frames_dir,frames_list[570])\n",
    "frame_path_8 = os.path.join(frames_dir,frames_list[575])\n",
    "frame_path_9 = os.path.join(frames_dir,frames_list[580])\n",
    "frame_path_10 = os.path.join(frames_dir,frames_list[585])\n",
    "frame_path_11 = os.path.join(frames_dir,frames_list[590])\n",
    "frame_path_12 = os.path.join(frames_dir,frames_list[595])\n",
    "frame_path_13 = os.path.join(frames_dir,frames_list[600])\n",
    "frame_path_14 = os.path.join(frames_dir,frames_list[605])\n",
    "\n",
    "# Create frames directory\n",
    "frame_paths = [frame_path_1, frame_path_2, frame_path_3, frame_path_4, frame_path_5, \n",
    "                frame_path_6, frame_path_7, frame_path_8, frame_path_9, frame_path_10, \n",
    "                frame_path_11, frame_path_12, frame_path_13, frame_path_14]\n",
    "\n",
    "# Create frames dictionary to hold each frame and the pedestrians within it\n",
    "frames_dict = dict()\n",
    "\n",
    "# Frame counter \n",
    "frame_count = 0\n",
    "\n",
    "# Begin loop through frames in frame directory\n",
    "for frame_path in frame_paths:\n",
    "    \n",
    "    # Set plot details for visulisation\n",
    "    _, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    \n",
    "    # Open cv analyses frame data\n",
    "    frame = cv2.imread(frame_path)\n",
    "    \n",
    "    # Run object detection\n",
    "    results = model.detect([frame], verbose=0)\n",
    "    \n",
    "    # collect results\n",
    "    r = results[0]\n",
    "    \n",
    "    # Collect labels for display\n",
    "    labels =[]\n",
    "    \n",
    "    # Create object dictionary to hold objects detected within results\n",
    "    objects_dict = dict()\n",
    "    \n",
    "    # Objects counter\n",
    "    objects_count = 0\n",
    "    \n",
    "    # Pedestrian counter\n",
    "    pedestrian_count = 0\n",
    "    \n",
    "    # Loop through all identified class id's \n",
    "    for id in r['class_ids']:\n",
    "        \n",
    "        # If class id is equal to 1 its a person\n",
    "        if id == 1:\n",
    "            \n",
    "            # Instantiate Centroid Tracker class\n",
    "            ct = CentroidTracker()\n",
    "            \n",
    "            # Add person plus person count to labels for visulisation\n",
    "            labels.append(str(pedestrian_count))\n",
    "            \n",
    "            # create collection for coordiunates for centroid calculation\n",
    "            rects = []\n",
    "            \n",
    "            # Set Coordinates of pedestrian for centroid calculation\n",
    "            # x1 = top\n",
    "            top = r['rois'][objects_count][1]\n",
    "            # y1 = left\n",
    "            left = r['rois'][objects_count][0]\n",
    "            # x2 = width\n",
    "            width = r['rois'][objects_count][3]\n",
    "            # y2 = height\n",
    "            height = r['rois'][objects_count][2]\n",
    "            \n",
    "            # Add coordinates to rects \n",
    "            rects.append([left, top, left + width, top + height])\n",
    "            \n",
    "            # Get centroid results for pedestrian returns: (id, array([ x, y])) id not working so ignore\n",
    "            centroid = ct.update(rects)\n",
    "            \n",
    "            # Get centroid X and Y\n",
    "            X = centroid[0][0]\n",
    "            Y = centroid[0][1]\n",
    "            \n",
    "            # Add person object to objects dictionary with centroid coordinates\n",
    "            objects_dict[objects_count] = {'pedestrian ' + str(pedestrian_count): (X, Y)}\n",
    "            \n",
    "            # Add 1 to pedestrian total count\n",
    "            pedestrian_count += 1\n",
    "        \n",
    "        # Else conditional for all other objects detected\n",
    "        else:\n",
    "            # Adds empty string to labels\n",
    "            labels.append(\"\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Add 1 to objects total count\n",
    "        objects_count += 1\n",
    "    \n",
    "    # Concatonate the frame number from frame path \n",
    "    frame_number = frame_path.replace(\"/home/sam/Documents/GitHub/honoursProject/4/frames/\", \"\").replace(\".jpg\", \"\")\n",
    "    \n",
    "    # Add frame with frame number to frames dictionary with the objects dictionary nested\n",
    "    frames_dict[frame_count] = {'frame ' + frame_number : objects_dict}\n",
    "\n",
    "    image_ir = visualize.display_instances(frame, r['rois'], r['masks'], r['class_ids'], \n",
    "                                class_names, r['scores'], ax=ax,\n",
    "                                title=\"Frame \" + str(frame_number), captions=labels)\n",
    "    \n",
    "    plt.savefig(os.path.join(output_5_dir,frame_number), bbox_inches='tight')\n",
    "    \n",
    "    # Add 1 to frames total count\n",
    "    frame_count += 1\n",
    "\n",
    "# Loop through frames dictionary\n",
    "for fd in frames_dict:\n",
    "    # Print the frame data for each dictionary\n",
    "    print(frames_dict[fd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create paths to frames \n",
    "frame_path_1 = os.path.join(frames_dir,frames_list[520])\n",
    "frame_path_2 = os.path.join(frames_dir,frames_list[530])\n",
    "frame_path_3 = os.path.join(frames_dir,frames_list[540])\n",
    "frame_path_4 = os.path.join(frames_dir,frames_list[550])\n",
    "frame_path_5 = os.path.join(frames_dir,frames_list[560])\n",
    "frame_path_6 = os.path.join(frames_dir,frames_list[570])\n",
    "frame_path_7 = os.path.join(frames_dir,frames_list[580])\n",
    "frame_path_8 = os.path.join(frames_dir,frames_list[590])\n",
    "frame_path_9 = os.path.join(frames_dir,frames_list[600])\n",
    "frame_path_10 = os.path.join(frames_dir,frames_list[610])\n",
    "frame_path_11 = os.path.join(frames_dir,frames_list[620])\n",
    "frame_path_12 = os.path.join(frames_dir,frames_list[630])\n",
    "frame_path_13 = os.path.join(frames_dir,frames_list[640])\n",
    "frame_path_14 = os.path.join(frames_dir,frames_list[650])\n",
    "\n",
    "# Create frames directory\n",
    "frame_paths = [frame_path_1, frame_path_2, frame_path_3, frame_path_4, frame_path_5, \n",
    "                frame_path_6, frame_path_7, frame_path_8, frame_path_9, frame_path_10, \n",
    "                frame_path_11, frame_path_12, frame_path_13, frame_path_14]\n",
    "\n",
    "# Create frames dictionary to hold each frame and the pedestrians within it\n",
    "frames_dict = dict()\n",
    "\n",
    "# Frame counter \n",
    "frame_count = 0\n",
    "\n",
    "# Begin loop through frames in frame directory\n",
    "for frame_path in frame_paths:\n",
    "    \n",
    "    # Set plot details for visulisation\n",
    "    _, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    \n",
    "    # Open cv analyses frame data\n",
    "    frame = cv2.imread(frame_path)\n",
    "    \n",
    "    # Run object detection\n",
    "    results = model.detect([frame], verbose=0)\n",
    "    \n",
    "    # collect results\n",
    "    r = results[0]\n",
    "    \n",
    "    # Collect labels for display\n",
    "    labels =[]\n",
    "    \n",
    "    # Create object dictionary to hold objects detected within results\n",
    "    objects_dict = dict()\n",
    "    \n",
    "    # Objects counter\n",
    "    objects_count = 0\n",
    "    \n",
    "    # Pedestrian counter\n",
    "    pedestrian_count = 0\n",
    "    \n",
    "    # Loop through all identified class id's \n",
    "    for id in r['class_ids']:\n",
    "        \n",
    "        # If class id is equal to 1 its a person\n",
    "        if id == 1:\n",
    "            \n",
    "            # Instantiate Centroid Tracker class\n",
    "            ct = CentroidTracker()\n",
    "            \n",
    "            # Add person plus person count to labels for visulisation\n",
    "            labels.append(str(pedestrian_count))\n",
    "            \n",
    "            # create collection for coordiunates for centroid calculation\n",
    "            rects = []\n",
    "            \n",
    "            # Set Coordinates of pedestrian for centroid calculation\n",
    "            # x1 = top\n",
    "            top = r['rois'][objects_count][1]\n",
    "            # y1 = left\n",
    "            left = r['rois'][objects_count][0]\n",
    "            # x2 = width\n",
    "            width = r['rois'][objects_count][3]\n",
    "            # y2 = height\n",
    "            height = r['rois'][objects_count][2]\n",
    "            \n",
    "            # Add coordinates to rects \n",
    "            rects.append([left, top, left + width, top + height])\n",
    "            \n",
    "            # Get centroid results for pedestrian returns: (id, array([ x, y])) id not working so ignore\n",
    "            centroid = ct.update(rects)\n",
    "            \n",
    "            # Get centroid X and Y\n",
    "            X = centroid[0][0]\n",
    "            Y = centroid[0][1]\n",
    "            \n",
    "            # Add person object to objects dictionary with centroid coordinates\n",
    "            objects_dict[objects_count] = {'pedestrian ' + str(pedestrian_count): (X, Y)}\n",
    "            \n",
    "            # Add 1 to pedestrian total count\n",
    "            pedestrian_count += 1\n",
    "        \n",
    "        # Else conditional for all other objects detected\n",
    "        else:\n",
    "            # Adds empty string to labels\n",
    "            labels.append(\"\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Add 1 to objects total count\n",
    "        objects_count += 1\n",
    "    \n",
    "    # Concatonate the frame number from frame path \n",
    "    frame_number = frame_path.replace(\"/home/sam/Documents/GitHub/honoursProject/4/frames/\", \"\").replace(\".jpg\", \"\")\n",
    "    \n",
    "    # Add frame with frame number to frames dictionary with the objects dictionary nested\n",
    "    frames_dict[frame_count] = {'frame ' + frame_number : objects_dict}\n",
    "\n",
    "    image_ir = visualize.display_instances(frame, r['rois'], r['masks'], r['class_ids'], \n",
    "                                class_names, r['scores'], ax=ax,\n",
    "                                title=\"Frame \" + str(frame_number), captions=labels)\n",
    "    \n",
    "    plt.savefig(os.path.join(output_10_dir,frame_number), bbox_inches='tight')\n",
    "    \n",
    "    # Add 1 to frames total count\n",
    "    frame_count += 1\n",
    "\n",
    "# Loop through frames dictionary\n",
    "for fd in frames_dict:\n",
    "    # Print the frame data for each dictionary\n",
    "    print(frames_dict[fd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataset from the 3 pedestrians movments \n",
    "\n",
    "# Pedestrian 0\n",
    "person_0_X = [767, 752, 746, 716, 717, 711, 699, 690, 689, 689]\n",
    "\n",
    "person_0_Y = [1243, 1217, 1188, 1148, 1126, 1101, 1078, 1073, 1065, 1058]\n",
    "\n",
    "# Pedestrian 1\n",
    "person_1_X = [803, 776, 767, 759, 742, 741, 736, 722, 719, 714]\n",
    "\n",
    "person_1_Y = [1368, 1324, 1295, 1266, 1235, 1217, 1191, 1171, 1154, 1138]\n",
    "\n",
    "# Pedestrian 2\n",
    "person_2_X = [782, 774, 789, 766, 760, 753, 739, 739, 725, 729]\n",
    "\n",
    "person_2_Y = [1452, 1422, 1367, 1332, 1305, 1271, 1246, 1228, 1212, 1186]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What should happen next\n",
    "\n",
    "The above would be repeated for each persons X and Y coordinates giving a prediciton for there positoions 10 frames in future\n",
    "\n",
    "The frame would be anaylsed by MRCNN\n",
    "\n",
    "Bounding box centroid compared to memspectrum average \n",
    "\n",
    "Have picture with dot from guess and dot from bounding box \n",
    "\n",
    "#### outputted results\n",
    "\n",
    "Pedestrians in the outputted frames are in the same order in each picture but the identification will identify them in a different order\n",
    "\n",
    "person original order is 0 1 2 I will put the correct order under each output:\n",
    "\n",
    "\n",
    "{'frame 540': {0: {'pedestrian 0': (767, 1243)}, 1: {'pedestrian 1': (803, 1368)}, 3: {'pedestrian 2': (782, 1452)}}}\n",
    "\n",
    "0 1 2\n",
    "\n",
    "{'frame 550': {0: {'pedestrian 0': (752, 1217)}, 1: {'pedestrian 1': (776, 1324)}, 2: {'pedestrian 2': (774, 1422)}}}\n",
    "\n",
    "0 1 2\n",
    "\n",
    "{'frame 560': {0: {'pedestrian 0': (789, 1367)}, 1: {'pedestrian 1': (746, 1188)}, 2: {'pedestrian 2': (767, 1295)}}}\n",
    "\n",
    "1 2 0\n",
    "\n",
    "{'frame 570': {0: {'pedestrian 0': (759, 1266)}, 1: {'pedestrian 1': (766, 1332)}, 2: {'pedestrian 2': (716, 1148)}}}\n",
    "\n",
    "2 0 1\n",
    "\n",
    "{'frame 580': {0: {'pedestrian 0': (742, 1235)}, 1: {'pedestrian 1': (717, 1126)}, 2: {'pedestrian 2': (760, 1305)}}}\n",
    "\n",
    "1 0 2\n",
    "\n",
    "{'frame 590': {0: {'pedestrian 0': (741, 1217)}, 1: {'pedestrian 1': (753, 1271)}, 2: {'pedestrian 2': (711, 1101)}}}\n",
    "\n",
    "2 0 1\n",
    "\n",
    "{'frame 600': {0: {'pedestrian 0': (736, 1191)}, 1: {'pedestrian 1': (739, 1246)}, 2: {'pedestrian 2': (699, 1078)}}}\n",
    "\n",
    "2 0 1\n",
    "\n",
    "{'frame 610': {0: {'pedestrian 0': (739, 1228)}, 1: {'pedestrian 1': (722, 1171)}, 2: {'pedestrian 2': (690, 1073)}}}\n",
    "\n",
    "2 1 0\n",
    "\n",
    "{'frame 620': {0: {'pedestrian 0': (725, 1212)}, 1: {'pedestrian 1': (719, 1154)}, 2: {'pedestrian 2': (689, 1065)}}}\n",
    "\n",
    "2 1 0\n",
    "\n",
    "{'frame 630': {0: {'pedestrian 0': (714, 1138)}, 1: {'pedestrian 1': (729, 1186)}, 2: {'pedestrian 2': (689, 1058)}}}\n",
    "\n",
    "2 0 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfdfb253d56b3adda966a24c23cbff432cd99a2a96e5b72ef76cc71805036f62"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
