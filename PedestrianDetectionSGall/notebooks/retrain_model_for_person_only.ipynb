{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain Model to accept only person class\n",
    "\n",
    "### To extract just pedestrian data from frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook currently does not work and sticks on first training epoch when using updated mrcnn from https://github.com/leekunhee/Mask_RCNN\n",
    "\n",
    "When investigated the identification from this model does not work the same as matterports original \n",
    "\n",
    "When trying to retrain matterports original model, errors are encountered due to changes within versions of Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Directory:  /home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "print(\"Root Directory: \", ROOT_DIR)\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "from scripts import coco\n",
    "from scripts.coco import Config\n",
    "from scripts import list_file_info as lfi\n",
    "\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/1/video/\"\n",
    "frames_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/1/frames/\"\n",
    "output_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/1/output_vid/\"\n",
    "final_imgs_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/1/output_img/\"\n",
    "json_output_dir = \"/home/sam/Desktop/sam_bdd_videos/videos_for_frames/1/output_json/\"\n",
    "COCO_DIR = \"/home/sam/Desktop/cocoData/\"\n",
    "COCO_MODEL_PATH = \"/home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/weights/mask_rcnn_coco.h5\"\n",
    "MODEL_DIR = \"/home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/weights/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     2\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 2\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           person\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    #GPU_COUNT = 1\n",
    "    #IMAGES_PER_GPU = 1\n",
    "\n",
    "# Alt Config to train own model on reduced classes \n",
    "class PersonConfig(Config):\n",
    "    NAME = \"person\"\n",
    "    NUM_CLASSES = 1 + 1 # background + sheep\n",
    "\n",
    "config = PersonConfig()  # Don't forget to use this config while creating your model\n",
    "config.display()\n",
    "\n",
    "\n",
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=8.17s)\n",
      "creating index...\n",
      "index created!\n",
      "[]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "ct = COCO(\"/home/sam/Desktop/cocoData/annotations/instances_train2014.json\")\n",
    "print(ct.getCatIds(['BG']))\n",
    "print(ct.getCatIds(['person']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=8.11s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.11s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Training dataset\n",
    "dataset_train = coco.CocoDataset()\n",
    "dataset_train.load_coco(COCO_DIR, \"train\", class_ids=[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = coco.CocoDataset()\n",
    "dataset_val.load_coco(COCO_DIR, \"val\", class_ids=[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None), name='tf.math.truediv/truediv:0', description=\"created by layer 'tf.math.truediv'\") of unsupported type <class 'keras.engine.keras_tensor.KerasTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/notebooks/pedestrian_movement_logging.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/notebooks/pedestrian_movement_logging.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39m# Create model in training mode\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/notebooks/pedestrian_movement_logging.ipynb#ch0000007?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m modellib\u001b[39m.\u001b[39;49mMaskRCNN(mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m\"\u001b[39;49m, config\u001b[39m=\u001b[39;49mconfig, model_dir\u001b[39m=\u001b[39;49mMODEL_DIR)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/notebooks/pedestrian_movement_logging.ipynb#ch0000007?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mload_weights(COCO_MODEL_PATH, by_name\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exclude\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmrcnn_class_logits\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmrcnn_bbox_fc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmrcnn_bbox\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmrcnn_mask\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/GitHub/honoursProject/PedestrianDetectionSGall/mrcnn/model.py:1841\u001b[0m, in \u001b[0;36mMaskRCNN.__init__\u001b[0;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[1;32m   <a href='file:///home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/mrcnn/model.py?line=1838'>1839</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_dir \u001b[39m=\u001b[39m model_dir\n\u001b[1;32m   <a href='file:///home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/mrcnn/model.py?line=1839'>1840</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_log_dir()\n\u001b[0;32m-> <a href='file:///home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/mrcnn/model.py?line=1840'>1841</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeras_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild(mode\u001b[39m=\u001b[39;49mmode, config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/Documents/GitHub/honoursProject/PedestrianDetectionSGall/mrcnn/model.py:1879\u001b[0m, in \u001b[0;36mMaskRCNN.build\u001b[0;34m(self, mode, config)\u001b[0m\n\u001b[1;32m   <a href='file:///home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/mrcnn/model.py?line=1875'>1876</a>\u001b[0m input_gt_boxes \u001b[39m=\u001b[39m KL\u001b[39m.\u001b[39mInput(\n\u001b[1;32m   <a href='file:///home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/mrcnn/model.py?line=1876'>1877</a>\u001b[0m     shape\u001b[39m=\u001b[39m[\u001b[39mNone\u001b[39;00m, \u001b[39m4\u001b[39m], name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput_gt_boxes\u001b[39m\u001b[39m\"\u001b[39m, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m   <a href='file:///home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/mrcnn/model.py?line=1877'>1878</a>\u001b[0m \u001b[39m# Normalize coordinates\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/mrcnn/model.py?line=1878'>1879</a>\u001b[0m gt_boxes \u001b[39m=\u001b[39m KL\u001b[39m.\u001b[39;49mLambda(\u001b[39mlambda\u001b[39;49;00m x: norm_boxes_graph(\n\u001b[1;32m   <a href='file:///home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/mrcnn/model.py?line=1879'>1880</a>\u001b[0m     x, K\u001b[39m.\u001b[39;49mshape(input_image)[\u001b[39m1\u001b[39;49m:\u001b[39m3\u001b[39;49m]))(input_gt_boxes)\n\u001b[1;32m   <a href='file:///home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/mrcnn/model.py?line=1880'>1881</a>\u001b[0m \u001b[39m# 3. GT Masks (zero padded)\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/mrcnn/model.py?line=1881'>1882</a>\u001b[0m \u001b[39m# [batch, height, width, MAX_GT_INSTANCES]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/sam/Documents/GitHub/honoursProject/PedestrianDetectionSGall/mrcnn/model.py?line=1882'>1883</a>\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mUSE_MINI_MASK:\n",
      "File \u001b[0;32m~/anaconda3/envs/mrcnn/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/sam/anaconda3/envs/mrcnn/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/sam/anaconda3/envs/mrcnn/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///home/sam/anaconda3/envs/mrcnn/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///home/sam/anaconda3/envs/mrcnn/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/sam/anaconda3/envs/mrcnn/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/mrcnn/lib/python3.9/site-packages/tensorflow/python/framework/type_spec.py:890\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    <a href='file:///home/sam/anaconda3/envs/mrcnn/lib/python3.9/site-packages/tensorflow/python/framework/type_spec.py?line=885'>886</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/sam/anaconda3/envs/mrcnn/lib/python3.9/site-packages/tensorflow/python/framework/type_spec.py?line=886'>887</a>\u001b[0m   logging\u001b[39m.\u001b[39mvlog(\n\u001b[1;32m    <a href='file:///home/sam/anaconda3/envs/mrcnn/lib/python3.9/site-packages/tensorflow/python/framework/type_spec.py?line=887'>888</a>\u001b[0m       \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to convert \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m to tensor: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, e))\n\u001b[0;32m--> <a href='file:///home/sam/anaconda3/envs/mrcnn/lib/python3.9/site-packages/tensorflow/python/framework/type_spec.py?line=889'>890</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not build a TypeSpec for \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/sam/anaconda3/envs/mrcnn/lib/python3.9/site-packages/tensorflow/python/framework/type_spec.py?line=890'>891</a>\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsupported type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None), name='tf.math.truediv/truediv:0', description=\"created by layer 'tf.math.truediv'\") of unsupported type <class 'keras.engine.keras_tensor.KerasTensor'>."
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(dataset_train, dataset_val,\n",
    "        learning_rate=config.LEARNING_RATE, \n",
    "        epochs=100, \n",
    "        layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"weights/mask_rcnn_coco.h5\")\n",
    "\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "class_names = ['BG','person']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "print(\"Loading weights \", COCO_MODEL_PATH)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection for frame names in frames_dir\n",
    "\n",
    "frames_list = lfi.get_file_names(frames_dir)\n",
    "\n",
    "# Sort the frames into accending order\n",
    "\n",
    "frames_list.sort(key=lambda f: int(re.sub('\\D', '', f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_annos = dict()\n",
    "\n",
    "\n",
    "for i in range(9):\n",
    "    \n",
    "    image_path = os.path.join(frames_dir,frames_list[i]) \n",
    "    print(image_path)\n",
    "    image = cv2.imread(image_path)\n",
    "    _, ax = plt.subplots(1, figsize=(26, 26))\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    results = model.detect([image], verbose=1)\n",
    "    \n",
    "    r = results[0]\n",
    "    \n",
    "    image_ir = visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                class_names, r['scores'], ax=ax,\n",
    "                                title=\"Predictions\")\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir,frames_list[i]), bbox_inches='tight')\n",
    "    \n",
    "    test_annos[i] = {frames_list[i] : {'objects': ([class_names[predicted_class_name] for predicted_class_name in r['class_ids']], \n",
    "                                                r['scores'].tolist(),  \n",
    "                                                r['rois'].tolist(),\n",
    "                                                class_names,\n",
    "                                                width, height)}}\n",
    "    if i==9:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfdfb253d56b3adda966a24c23cbff432cd99a2a96e5b72ef76cc71805036f62"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
